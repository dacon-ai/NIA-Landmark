{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=1., train_p=False):\n",
    "        super().__init__()\n",
    "        if train_p:\n",
    "            self.p = tf.Variable(p, dtype=tf.float32)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, **kwargs):\n",
    "        inputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
    "        inputs = tf.pow(inputs, self.p)\n",
    "        inputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
    "        inputs = tf.pow(inputs, 1./self.p)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class DelfArcFaceModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, n_classes, margin, logit_scale, feature_size, p=None, train_p=False):\n",
    "        super().__init__()\n",
    "        self.backbone = tf.keras.applications.ResNet101(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "        #elf.backbone.summary()\n",
    "\n",
    "        if p is not None:\n",
    "            self.global_pooling = GeMPoolingLayer(p, train_p=train_p)\n",
    "        else:\n",
    "            self.global_pooling = functools.partial(tf.reduce_mean, axis=[1, 2], keepdims=False)\n",
    "        self.dense1 = tf.keras.layers.Dense(feature_size, activation='softmax', kernel_initializer=\"glorot_normal\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.Flatten = tf.keras.layers.Flatten()\n",
    "        self.arcface = ArcFaceLayer(n_classes, margin, logit_scale)\n",
    "        \n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        images, labels = inputs\n",
    "        x = self.extract_feature(images)\n",
    "        x = self.arcface((x, labels))\n",
    "        return x\n",
    "        \n",
    "    def extract_feature(self, inputs):\n",
    "        x = self.backbone(inputs)\n",
    "        x = self.global_pooling(x)\n",
    "        x = self.Flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ArcFaceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, margin, logit_scale):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.logit_scale = logit_scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\"weights\", shape=[int(input_shape[0][-1]), self.num_classes], initializer=tf.keras.initializers.get(\"glorot_normal\"))\n",
    "        self.cos_m = tf.identity(tf.cos(self.margin), name='cos_m')\n",
    "        self.sin_m = tf.identity(tf.sin(self.margin), name='sin_m')\n",
    "        self.th = tf.identity(tf.cos(math.pi - self.margin), name='th')\n",
    "        self.mm = tf.multiply(self.sin_m, self.margin, name='mm')\n",
    "\n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        embeddings, labels = inputs\n",
    "        normed_embeddings = tf.nn.l2_normalize(embeddings, axis=1, name='normed_embd')\n",
    "        normed_w = tf.nn.l2_normalize(self.w, axis=0, name='normed_weights')\n",
    "\n",
    "        cos_t = tf.matmul(normed_embeddings, normed_w, name='cos_t')\n",
    "        sin_t = tf.sqrt(1. - cos_t ** 2, name='sin_t')\n",
    "\n",
    "        cos_mt = tf.subtract(cos_t * self.cos_m, sin_t * self.sin_m, name='cos_mt')\n",
    "\n",
    "        cos_mt = tf.where(cos_t > self.th, cos_mt, cos_t - self.mm)\n",
    "\n",
    "        mask = tf.one_hot(tf.cast(labels, tf.int32), depth=self.num_classes,\n",
    "                          name='one_hot_mask')\n",
    "\n",
    "        logits = tf.where(mask == 1., cos_mt, cos_t)\n",
    "        logits = tf.multiply(logits, self.logit_scale, 'arcface_logist')\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "batch_size = 64\n",
    "image_size = 224\n",
    "STEPS_PER_TPU_CALL = 1\n",
    "learning_rate = 5e-5  # should be smaller than training on single GPU\n",
    "feature_size = 2048  # Embedding size before the output layer\n",
    "save_interval = 2000\n",
    "\n",
    "# ArcFace params\n",
    "margin = 0.1  # DELG used 0.1, original ArcFace paper used 0.5. When margin is 0, it should be the same as doing a normal softmax but with embedding and weight normalised.\n",
    "logit_scale = int(math.sqrt(feature_size))\n",
    "\n",
    "# GeM params\n",
    "gem_p = 3.\n",
    "train_p = False  # whether to learn gem_p or not\n",
    "data_dirs = '/home/ubuntu/Dacon/cpt_data/landmark/'\n",
    "data_dir = \"/home/ubuntu/Dacon/jin/NIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/home/ubuntu/Dacon/jin/NIA/checkpoint/\"\n",
    "train_tf_records_dir = \"/home/ubuntu/Dacon/jin/NIA/tfrecords/train*\"\n",
    "test_tf_records_dir = \"/home/ubuntu/Dacon/jin/NIA/tfrecords/validation*\"\n",
    "training_csv_path = os.path.join(data_dir, \"train.csv\")\n",
    "train_csv = pd.read_csv(str(training_csv_path))\n",
    "num_samples = len(train_csv[\"id\"].tolist())\n",
    "unique_landmark_ids = train_csv[\"landmark_id\"].unique().tolist()\n",
    "unique_landmark_ids = tf.convert_to_tensor(unique_landmark_ids, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7488 images belonging to 309 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = test_datagen.flow_from_directory(\n",
    "    directory = data_dirs + 'test/',\n",
    "    class_mode = 'sparse',\n",
    "    shuffle = True,\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='eval_accuracy')\n",
    "\n",
    "model = DelfArcFaceModel(\n",
    "            input_shape=(image_size, image_size, 3), n_classes=len(unique_landmark_ids), margin=margin, logit_scale=logit_scale,\n",
    "            p=gem_p, train_p=train_p, feature_size=feature_size\n",
    "        )\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "  predictions = model((images, labels), training=False)\n",
    "  eval_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f14312fb6d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer delf_arc_face_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  predictions = model((images, dummy_labels), training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_labels = np.zeros(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 71,  71,  71,  71, 172,  71,  71,  71,  71,  71,  71,  71, 172,\n",
       "        71,  71,  71, 172,  71,  71,  71,  71,  71,  71,  71,  71,  71,\n",
       "        71,  71,  71,  71,  71,  71,  71,  71,  71,  71,  71,  71,  71,\n",
       "       172,  71,  71,  71,  71,  71,  71,  71,  71, 172,  71,  71,  71,\n",
       "        71,  71,  71,  71,  71,  71,  71, 172,  71,  71,  71,  71])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_data:\n",
    "  eval_step(images, labels)\n",
    "  print(eval_accuracy.result()*100)\n",
    "print ('전략을 사용하지 않고, 저장된 모델을 복원한 후의 정확도: {}'.format(\n",
    "    eval_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landmark",
   "language": "python",
   "name": "landmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
